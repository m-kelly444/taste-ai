#!/bin/bash
set -e  # Exit on error

# Script: Cleanup repository and optimize for Claude collaboration
# Usage: ./cleanup_repo.sh

echo "🧹 Scanning repository for large files and optimization opportunities..."

# Create .gitignore if it doesn't exist
if [ ! -f .gitignore ]; then
    echo "Creating .gitignore file..."
    cat > .gitignore << 'EOF'
# Data directories - these can get large
data/
*.json
*.html
*.csv
*.xlsx
*.xls

# Python cache
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
pip-log.txt
pip-delete-this-directory.txt

# Virtual environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE files
.vscode/
.idea/
*.swp
*.swo
*~

# OS files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
*.log
logs/

# Temporary files
tmp/
temp/
*.tmp
*.temp

# Large analysis files
*.png
*.jpg
*.jpeg
*.gif
*.svg
*.pdf

# Model files
*.pkl
*.model
*.h5
*.ckpt

# Browser data
*.sqlite
*.db

# Screenshots
screenshots/
EOF
    echo "✅ Created .gitignore"
fi

# Find large files in the repository
echo "🔍 Scanning for large files (>1MB)..."
find . -type f -size +1M -not -path "./.git/*" | while read file; do
    size=$(du -h "$file" | cut -f1)
    echo "📁 Large file: $file ($size)"
done

# Find data directories that might be large
echo "🔍 Checking data directories..."
if [ -d "./data" ]; then
    data_size=$(du -sh ./data 2>/dev/null | cut -f1)
    echo "📊 ./data directory size: $data_size"
    
    # Show what's in data directory
    echo "Contents of ./data:"
    find ./data -type f -exec du -h {} \; | sort -hr | head -10
fi

# Check if files are already tracked by git
echo "🔍 Checking which large files are tracked by git..."
git ls-files --others --ignored --exclude-standard | head -10

# Check current git status
echo "📊 Current git repository status:"
git status --porcelain | head -10

# Remove large files from git tracking (but keep locally)
echo "🚮 Removing large files from git tracking..."

# Remove data directory from git if it exists
if [ -d "./data" ]; then
    git rm -r --cached ./data 2>/dev/null || echo "./data not in git tracking"
fi

# Remove common large file types
for pattern in "*.json" "*.html" "*.csv" "*.xlsx" "*.log" "*.png" "*.jpg"; do
    git rm --cached $pattern 2>/dev/null || true
done

# Create a data directory structure with README files instead
echo "📁 Creating optimized directory structure..."
mkdir -p data/{portfolio,analysis,images,logs}

# Create README files to document what goes where
cat > data/README.md << 'EOF'
# Data Directory

This directory contains all the scraped and analyzed data. Files here are gitignored to keep the repo small.

## Structure:
- `portfolio/` - Company data and portfolio information
- `analysis/` - ML analysis results and visual analysis
- `images/` - Downloaded product images and screenshots  
- `logs/` - Execution logs and debug information

## Files typically found here:
- `companies.json` - Extracted company information
- `portfolio_readable.html` - Scraped portfolio page
- `visual_analysis.json` - Computer vision analysis results
- Various image files from company websites
EOF

cat > data/portfolio/README.md << 'EOF'
# Portfolio Data

Files generated by portfolio scraping scripts:
- `portfolio_readable.html` - Raw HTML from portfolio page
- `companies.json` - Extracted company list  
- `companies_cleaned.json` - Cleaned company names
- `all_companies.json` - Complete extraction results
EOF

cat > data/analysis/README.md << 'EOF'
# Analysis Results

Files generated by analysis scripts:
- `visual_analysis.json` - Computer vision analysis of product images
- `ml_analysis.json` - Machine learning analysis results
- Various analysis output files
EOF

# Create a sample config file instead of hardcoding API keys
cat > config.sample.env << 'EOF'
# Copy this to config.env and fill in your API keys
GROK_API_KEY=your_grok_api_key_here
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here

# Add other configuration as needed
EOF

echo "config.env" >> .gitignore

# Update scripts to use relative paths and be more modular
echo "🔧 Creating optimized script structure..."

# Create a main runner script
cat > run_analysis.sh << 'EOF'
#!/bin/bash

# Main analysis runner - executes all steps in sequence
echo "🚀 Starting portfolio analysis pipeline..."

# Check if data directory exists
if [ ! -d "./data" ]; then
    mkdir -p data/{portfolio,analysis,images,logs}
fi

# Run scripts in sequence
echo "Step 1: Fetching portfolio page..."
./01_fetch_portfolio.sh 2>&1 | tee data/logs/01_fetch.log

echo "Step 2: Parsing portfolio..."  
./02_parse_portfolio.sh 2>&1 | tee data/logs/02_parse.log

echo "Step 3: Cleaning company names..."
./03_clean_company_names.sh 2>&1 | tee data/logs/03_clean.log

echo "Step 4: Deep extraction..."
./04_deep_portfolio_extraction.sh 2>&1 | tee data/logs/04_deep.log

echo "Step 5: Visual analysis..."
./05_real_visual_analysis.sh 2>&1 | tee data/logs/05_visual.log

echo "✅ Analysis pipeline complete!"
echo "📊 Check data/analysis/ for results"
EOF

chmod +x run_analysis.sh

# Add logs directory to gitignore
echo "data/logs/" >> .gitignore

# Create requirements.txt for Python dependencies
cat > requirements.txt << 'EOF'
requests==2.31.0
beautifulsoup4==4.12.2
opencv-python==4.8.1.78
Pillow==10.0.1
numpy==1.24.3
scikit-learn==1.3.0
scikit-image==0.21.0
selenium==4.15.0
textblob==0.17.1
pandas==2.0.3
lxml==4.9.3
EOF

# Update existing scripts to use the data directory properly
echo "🔧 Optimizing existing scripts..."

# Update the scripts to use better error handling and smaller outputs
for script in *.sh; do
    if [[ "$script" != "cleanup_repo.sh" && "$script" != "run_analysis.sh" ]]; then
        # Add error handling to scripts
        sed -i.bak '1a\
set -e  # Exit on error\
' "$script" 2>/dev/null || true
        
        # Remove backup files
        rm -f "${script}.bak" 2>/dev/null || true
    fi
done

# Commit the cleanup
echo "💾 Committing repository cleanup..."
git add .gitignore requirements.txt run_analysis.sh config.sample.env
git add data/README.md data/portfolio/README.md data/analysis/README.md 2>/dev/null || true

# Show final status
echo "📊 Final repository status:"
echo "Repository size:"
du -sh . --exclude=.git

echo "Files in repository (excluding .git):"
find . -type f -not -path "./.git/*" | wc -l

echo "Largest remaining files:"
find . -type f -not -path "./.git/*" -exec du -h {} \; | sort -hr | head -5

echo ""
echo "✅ Repository cleanup complete!"
echo "🔧 To continue working:"
echo "   1. Run: ./run_analysis.sh"
echo "   2. Copy config.sample.env to config.env and add your API keys"
echo "   3. Install Python deps: pip install -r requirements.txt"
echo "   4. All data will be stored in ./data/ (gitignored)"
echo ""
echo "📂 The repo is now optimized for Claude collaboration!"